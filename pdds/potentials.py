import abc
from check_shapes import check_shapes
import typing as tp

import jax
import jax.numpy as jnp

from jaxtyping import Array

from pdds.distributions import Distribution, NormalDistributionWrapper


class BasePotential(metaclass=abc.ABCMeta):
    """Abstract class for implementing potential at time 0, log_g0."""

    def __init__(self, sigma: float, target: Distribution):
        self.sigma = sigma
        self.target = target
        self.dim = target.dim

    @abc.abstractmethod
    @check_shapes("x: [b, d]", "return[0]: [b]")
    def _log_g0(self, x: Array, density_state: int) -> tp.Tuple[Array, int]:
        """Evaluates log_g0.

        Args:
            x: Samples
            density_state: int, tracks target density evaluations
        Returns:
            Array of shape [b,] containing log_g0.
            Int containing updated density state."""
        pass


class RatioPotential(BasePotential):
    """Evaluates log_g0 in the standard case log(target/reference)"""

    @check_shapes("x: [b, d]", "return[0]: [b]")
    def _log_g0(self, x: Array, density_state: int) -> tp.Tuple[Array, int]:
        target_density, density_state = self.target.evaluate_log_density(
            x, density_state
        )
        normal_dist = NormalDistributionWrapper(mean=0, scale=self.sigma, dim=self.dim)
        ref_density, density_state = normal_dist.evaluate_log_density(x, density_state)
        return target_density - ref_density, density_state


class ConstantPotential(BasePotential):
    """Evaluates log_g0 in the case target=reference, useful for testing"""

    @check_shapes("x: [b, d]", "return[0]: [b]")
    def _log_g0(self, x: Array, density_state: int) -> tp.Tuple[Array, int]:
        return jnp.ones(shape=x.shape[:-1]), density_state


# Base class for potential functions g_t, either analytical or approximations eg naive or NN
class ApproxPotential(metaclass=abc.ABCMeta):
    r"""Represents the log-potential function :math:`\log g_0` at time 0, as well as approximations of the logarithm of
        .. math:: g_t(x_t) = \int p(x_0|x_t) g_0(x_0) dx_0
    where the notation p refers to the distribution generated by the SDE :math:`dX_t = -\beta_t X_t dt + \sqrt{2 \beta_t \sigma^2} dW_t`.

    Warnings
    --------
    Since
        .. math:: p(x_0|x_t) \sim \mathcal N(\sqrt{1-\lambda} x_t, \lambda \sigma^2)
    for a suitable :math:`\lambda`, any approximation of :math:`g_t` will depend only on :math:`\lambda` and :math:`\sigma`. The function ``approx_log_gt`` will be parametrised as such.
    """

    def __init__(
        self, base_potential: BasePotential, dim: int, nn_potential_approximator=None
    ):
        r"""
        Parameters
        ----------
        dim
            dimension of the problem
        log_g0
            vectorised implementation of the mathematical :math:`\log g_0`. Accepts input array of shape ``(N, dim)`` for any `N`, returns array of shape ``(N,)``.
        nn_potential_approximator:
            callable function giving neural network approximation to potential function
        """
        self.base_potential = base_potential
        self.log_g0 = base_potential._log_g0
        self.nn_potential_approximator = nn_potential_approximator
        self.dim = dim

    @abc.abstractmethod
    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        ...


# Naive approximation of the potential g_t = g_0(E(X_0|x_t))
class NaivelyApproximatedPotential(ApproxPotential):
    """Implements the simple approximation of the log-potential function, that is:
    .. math:: g_t(x_t) = \int p(x_0|x_t) g_0(x_0) dx_0 \approx g_0(\sqrt{1-\lambda_t} * x)
    """

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        if len(x.shape) == 1:
            x = jnp.expand_dims(x, 0)
            out, density_state = self.log_g0(
                jnp.sqrt(1 - lbd)[..., None] * x, density_state
            )
            return out[0], density_state
        else:
            return self.log_g0(jnp.sqrt(1 - lbd)[..., None] * x, density_state)


class NNApproximatedPotential(ApproxPotential):
    """Neural network potential approximator"""

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        return self.nn_potential_approximator(lbd=lbd, x=x, density_state=density_state)


# An analytical potential where the target and reference are the same
class ConstantAnalyticPotential(ApproxPotential):
    """Evaluates the log potential function g_t when target=reference, useful for testing."""

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        dim = x.shape[:-1]
        return jnp.zeros(shape=dim), density_state


class GaussianRatioAnalyticPotential(ApproxPotential):
    """Analytic log-potential function when the target and reference are Gaussians."""

    def __init__(
        self,
        base_potential: BasePotential,
        dim: int,
        sigma: float,
        nn_potential_approximator=None,
    ):
        super().__init__(base_potential, dim)
        assert dim == 1
        self.sigma = sigma

    @property
    def mu_ref(self):
        return 0.0

    @property
    def mu_target(self):
        return self.base_potential.target._mean

    @property
    def sigma_target(self):
        return self.base_potential.target._scale

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        lbd = lbd[..., None]

        def fun_0(args):
            x = args[0]
            density_state = args[-1]
            if len(x.shape) == 1:
                out, density_state = self.log_g0(x, density_state)
                return out[0], density_state
            else:
                return self.log_g0(x, density_state)

        def fun_other(args):
            x = args[0]
            lbd = args[1]
            sigma = args[2]
            density_state = args[3]
            reshaped = False
            if len(x.shape) == 1:
                reshaped = True
                x = jnp.expand_dims(x, 0)

            a = sigma**2 * lbd - self.sigma_target**2 * lbd + self.sigma_target**2
            b = (
                sigma**2 * lbd * self.mu_target
                - self.sigma_target**2 * lbd * self.mu_ref
                + self.sigma_target**2 * jnp.sqrt(1 - lbd) * x
            )
            c = (
                sigma**2 * lbd * self.mu_target**2
                - self.sigma_target**2 * lbd * self.mu_ref**2
                + (1 - lbd) * self.sigma_target**2 * x**2
            )
            log_potential = (
                jnp.log(sigma)
                - 0.5 * jnp.log(a)
                - (c - (b**2) / a) / (2 * sigma**2 * self.sigma_target**2 * lbd)
            )

            if reshaped:
                return log_potential[:, 0][0], density_state + 1
            else:
                return log_potential[:, 0], density_state + x.shape[0]

        return jax.lax.cond(
            (lbd[0] == jnp.array(0.0))[0],
            fun_0,
            fun_other,
            (x, lbd, self.sigma, density_state),
        )

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def log_pi(self, lbd: Array, x: Array, density_state: int) -> tp.Tuple[Array, int]:
        lbd = lbd[..., None]
        reshaped = False
        if len(x.shape) == 1:
            reshaped = True
            x = jnp.expand_dims(x, 0)

        t1 = -0.5 * jnp.log(2 * jnp.pi * (lbd + (1 - lbd) * self.sigma_target**2))
        t2 = -((x - jnp.sqrt(1 - lbd) * self.mu_target) ** 2) / (
            2 * (lbd + (1 - lbd) * self.sigma_target**2)
        )
        out = t1 + t2
        if reshaped:
            return out[:, 0][0], density_state + 1
        else:
            return out[:, 0], density_state + x.shape[0]

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def log_g_tilde(self, lbd: Array, x: Array, density_state) -> tp.Tuple[Array, int]:
        if len(x.shape) == 1:
            gt_term, density_state = self.approx_log_gt(lbd, x, density_state)
            g0_term, density_state = self.log_g0(
                jnp.sqrt(1 - lbd)[..., None] * jnp.expand_dims(x, 0), density_state
            )
            out = gt_term - g0_term
            return out[:, 0][0], density_state
        else:
            gt_term, density_state = self.approx_log_gt(lbd, x, density_state)
            g0_term, density_state = self.log_g0(
                jnp.sqrt(1 - lbd)[..., None] * x, density_state
            )
            return gt_term - g0_term, density_state


class GaussianRatioAnalyticPotentialWrongScale(ApproxPotential):
    """Analytic potential function with deliberately wrong scale, used in testing."""

    def __init__(
        self, base_potential: BasePotential, dim: int, sigma: float, corrector=None
    ):
        super().__init__(base_potential, dim)
        assert dim == 1
        self.sigma = sigma

    @property
    def mu_ref(self):
        return 0.0

    @property
    def mu_target(self):
        return self.base_potential.target._mean

    @property
    def sigma_target(self):
        return self.base_potential.target._scale

    @check_shapes("x: [b, d]", "lbd: [b]", "return[0]: [b]")
    def approx_log_gt(
        self, lbd: Array, x: Array, density_state: int
    ) -> tp.Tuple[Array, int]:
        lbd = lbd[..., None]

        def fun_0(args):
            x = args[0]
            if len(x.shape) == 1:
                return self.log_g0(x)[0]
            else:
                return self.log_g0(x)

        def fun_other(args):
            x = args[0]
            lbd = args[1]
            sigma = args[2]
            reshaped = False
            if len(x.shape) == 1:
                reshaped = True
                x = jnp.expand_dims(x, 0)

            a = sigma**2 * lbd - self.sigma_target**2 * lbd + self.sigma_target**2
            b = (
                sigma**2 * lbd * self.mu_target
                - self.sigma_target**2 * lbd * self.mu_ref
                + self.sigma_target**2 * jnp.sqrt(1 - lbd) * x
            )
            c = (
                sigma**2 * lbd * self.mu_target**2
                - self.sigma_target**2 * lbd * self.mu_ref**2
                + (1 - lbd) * self.sigma_target**2 * x**2
            )
            log_potential = (
                jnp.log(sigma)
                - 0.5 * jnp.log(a)
                - (c - (b**2) / a) / (2 * sigma**2 * self.sigma_target**2 * lbd)
            )

            if reshaped:
                return log_potential[:, 0][0] + 20 * lbd[0, 0]
            else:
                return log_potential[:, 0] + 20 * lbd[:, 0]

        return (
            jax.lax.cond(
                (lbd[0] == jnp.array(0.0))[0], fun_0, fun_other, (x, lbd, self.sigma)
            ),
            density_state + x.shape[0],
        )
